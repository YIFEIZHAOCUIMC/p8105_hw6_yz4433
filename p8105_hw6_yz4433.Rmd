---
title: "p8105_hw6_yz4433"
author: "Yifei Zhao"
date: "2022-11-29"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(rvest)
library(httr)
library(RCurl)
library(viridis)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

To obtain a distribution for $\hat{r}^2$, we'll follow basically the same procedure we used for regression coefficients: draw bootstrap samples; the a model to each; extract the value I'm concerned with; and summarize. Here, we'll use `modelr::bootstrap` to draw the samples and `broom::glance` to produce `r.squared` values. 

```{r weather_df, cache = TRUE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```


```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1 may be a cause for the generally skewed shape of the distribution. If we wanted to construct a confidence interval for $R^2$, we could take the 2.5% and 97.5% quantiles of the estimates across bootstrap samples. However, because the shape isn't symmetric, using the mean +/- 1.96 times the standard error probably wouldn't work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a similar approach, with a bit more wrangling before we make our plot.

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

As with $r^2$, this distribution is somewhat skewed and has some outliers. 

The point of this is not to say you should always use the bootstrap -- it's possible to establish "large sample" distributions for strange parameters / values / summaries in a lot of cases, and those are great to have. But it is helpful to know that there's a way to do inference even in tough cases. 

## Problem 2
### manipulate data
```{r}
hcdata =  read_csv(file = "./data/homicide-data.csv", show_col_types = FALSE) %>% 
  janitor::clean_names() %>% 
  mutate(city_state = paste(city,',',state)) %>% 
  mutate(group = ifelse((disposition == 'Closed by arrest'), 1, 0)) %>%
  filter(!city_state %in% c("Dallas , TX", "Phoenix , AZ", "Kansas City , MO", "Tulsa , AL")) %>% 
  filter(victim_race %in% c("Black", "White")) %>% 
  filter(victim_age != 'Unknown') %>% 
  mutate(victim_age = as.numeric(victim_age))
```

We manipulate data to meet requirements for further analysis.

### glm for Baltimore
```{r}
baldata = hcdata %>% 
  filter(city_state == "Baltimore , MD")

glmbal = glm(group ~ victim_age + victim_sex + victim_race, data = baldata, family = binomial)
```

```{r}
glmres = glmbal %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>% 
  mutate(ci_low = data.frame(exp(confint.default(glmbal, level = 0.95)))[[1]], ci_high = data.frame(exp(confint.default(glmbal, level = 0.95)))[[2]]) %>% 
  filter(term == "victim_sexMale") %>% 
  select(-5:-1)
glmres
```

### glm for each city
```{r}
ct = hcdata %>%
  group_by(city_state)
list_ct = group_split(ct)
```

```{r}
orci = function(x) {
  glmi = glm(group ~ victim_age + victim_sex + victim_race, data = x, family = binomial)
  glmi %>% 
    broom::tidy() %>% 
    mutate(city_state = x[[1,13]]) %>% 
    mutate(or = exp(estimate)) %>% 
    mutate(ci_low = data.frame(exp(confint.default(glmi, level = 0.95)))[[1]], ci_high = data.frame(exp(confint.default(glmi, level = 0.95)))[[2]]) %>% 
    filter(term == "victim_sexMale") %>%
    select(-5:-1) %>% 
    data.frame()
}
```

```{r}
output = map_dfr(list_ct, orci)
output
```

### plot of ORs and CIs for each city
```{r}
p = ggplot(output, aes(x = reorder(city_state, or), y = or, fill = city_state)) +
  geom_bar(stat = "identity", color = "black", position = position_dodge()) +
  geom_errorbar(aes(ymin = ci_low, ymax = ci_high), width = .2, position = position_dodge(.9)) +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "city name", y = "estimated ORs", title = "ORs and CIs for each city")
print(p)
```

From the plot, we find the estimated OR of city Stockton, CA is the largest, and that of New York, NY is the lowest. There is a huge gap between the largest OR and the second largest OR. For confidence interval, the range of CI of Stockton is the largest, considering the estimated OR, and that for San Bernardino, CA is also large compared with other close ORs.

## Problem 3
### load and clean the data
```{r}
bwdata =  read_csv(file = "./data/birthweight.csv", show_col_types = FALSE) %>% 
  janitor::clean_names() %>%
  drop_na() %>% 
  mutate(babysex = factor(as.character(babysex)), frace = factor(as.character(frace)), malform = factor(as.character(malform)), mrace = factor(as.character(mrace)))
```

```{r}
bwdata %>%
  summarise_all(
    ~ sum(is.na(.))
  )
```

We manipulate data to meet requirements for further analysis.

### my stepwise model
```{r}
lmbw = lm(bwt ~ ., data = bwdata)
swboth = step(lmbw, direction = 'both')
summary(swboth)
```

We initially create the multiple linear regression (MLR) model using all the variables as predictors. Finally we decide to apply stepwise method for the MLR to obtain the optimal model.

### add residuals and predictions
```{r}
bwdata %>% 
  modelr::add_predictions(swboth) %>% 
  modelr::add_residuals(swboth) %>% 
  ggplot(aes(x = resid, y = pred)) + geom_point() + ggtitle('residual-fitted')
```

### compare models
```{r}
cv_df = crossv_mc(bwdata, 100)
cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
cv_df = 
  cv_df %>% 
  mutate(
    my_mod = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
    a_mod  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    b_mod  = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(
    rmse_my = map2_dbl(my_mod, test, ~rmse(model = .x, data = .y)),
    rmse_a = map2_dbl(a_mod, test, ~rmse(model = .x, data = .y)),
    rmse_b = map2_dbl(b_mod, test, ~rmse(model = .x, data = .y)))
```

### comparison plot
```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() + ggtitle('model comparison')
```

Iâ€™m mostly focused on RMSE as a way to compare these models, and the plot above shows the distribution of RMSE values for each candidate model. We can clearly find that the MLR under stepwise model is the best among these three models, and model b with interaction factors is better that the simple MLR of model a.