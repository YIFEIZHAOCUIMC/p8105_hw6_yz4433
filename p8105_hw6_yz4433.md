p8105_hw6_yz4433
================
Yifei Zhao
2022-11-29

## Problem 1

To obtain a distribution for $\hat{r}^2$, we’ll follow basically the
same procedure we used for regression coefficients: draw bootstrap
samples; the a model to each; extract the value I’m concerned with; and
summarize. Here, we’ll use `modelr::bootstrap` to draw the samples and
`broom::glance` to produce `r.squared` values.

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

    ## Registered S3 method overwritten by 'hoardr':
    ##   method           from
    ##   print.cache_info httr

    ## file min/max dates: 1869-01-01 / 2022-11-30

``` r
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

<img src="p8105_hw6_yz4433_files/figure-gfm/unnamed-chunk-1-1.png" width="90%" />

In this example, the $\hat{r}^2$ value is high, and the upper bound at 1
may be a cause for the generally skewed shape of the distribution. If we
wanted to construct a confidence interval for $R^2$, we could take the
2.5% and 97.5% quantiles of the estimates across bootstrap samples.
However, because the shape isn’t symmetric, using the mean +/- 1.96
times the standard error probably wouldn’t work well.

We can produce a distribution for $\log(\beta_0 * \beta1)$ using a
similar approach, with a bit more wrangling before we make our plot.

``` r
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```

<img src="p8105_hw6_yz4433_files/figure-gfm/unnamed-chunk-2-1.png" width="90%" />

As with $r^2$, this distribution is somewhat skewed and has some
outliers.

The point of this is not to say you should always use the bootstrap –
it’s possible to establish “large sample” distributions for strange
parameters / values / summaries in a lot of cases, and those are great
to have. But it is helpful to know that there’s a way to do inference
even in tough cases.

## Problem 2

### manipulate data

``` r
hcdata =  read_csv(file = "./data/homicide-data.csv", show_col_types = FALSE) %>% 
  janitor::clean_names() %>% 
  mutate(city_state = paste(city,',',state)) %>% 
  mutate(group = ifelse((disposition == 'Closed by arrest'), 1, 0)) %>%
  filter(!city_state %in% c("Dallas , TX", "Phoenix , AZ", "Kansas City , MO", "Tulsa , AL")) %>% 
  filter(victim_race %in% c("Black", "White")) %>% 
  filter(victim_age != 'Unknown')
```

### glm for Baltimore

``` r
baldata = hcdata %>% 
  filter(city_state == "Baltimore , MD")

glmbal = glm(group ~ victim_age + victim_sex + victim_race, data = baldata, family = binomial)
```

``` r
glmres = glmbal %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate)) %>% 
  mutate(ci_low = data.frame(exp(confint.default(glmbal, level = 0.95)))[[1]], ci_high = data.frame(exp(confint.default(glmbal, level = 0.95)))[[2]]) %>% 
  filter(term == "victim_sexMale") %>% 
  select(-5:-1)
glmres
```

    ## # A tibble: 1 × 3
    ##      OR ci_low ci_high
    ##   <dbl>  <dbl>   <dbl>
    ## 1 0.456  0.340   0.612

### glm for each city

``` r
ct = hcdata %>%
  group_by(city_state)
list_ct = group_split(ct)
```

``` r
orci = function(x) {
  glmi = glm(group ~ victim_age + victim_sex + victim_race, data = x, family = binomial)
  glmi %>% 
    broom::tidy() %>% 
    mutate(city_state = x[[1,13]]) %>% 
    mutate(or = exp(estimate)) %>% 
    mutate(ci_low = data.frame(exp(confint.default(glmi, level = 0.95)))[[1]], ci_high = data.frame(exp(confint.default(glmi, level = 0.95)))[[2]]) %>% 
    filter(term == "victim_sexMale") %>%
    select(-5:-1) %>% 
    data.frame()
}
```

``` r
output = map_dfr(list_ct, orci)
output
```

    ##             city_state        or     ci_low    ci_high
    ## 1     Albuquerque , NM 1.2993250 0.39815643  4.2401562
    ## 2         Atlanta , GA 1.0276747 0.66528906  1.5874533
    ## 3       Baltimore , MD 0.4559900 0.33978176  0.6119425
    ## 4     Baton Rouge , LA 0.3251091 0.15882112  0.6655030
    ## 5      Birmingham , AL 0.7998153 0.49793147  1.2847239
    ## 6          Boston , MA 0.5339613 0.26119309  1.0915860
    ## 7         Buffalo , NY 0.4757883 0.23428429  0.9662384
    ## 8       Charlotte , NC 0.8334375 0.49331219  1.4080700
    ## 9         Chicago , IL 0.4816031 0.38755573  0.5984728
    ## 10     Cincinnati , OH 0.4063035 0.22242111  0.7422073
    ## 11       Columbus , OH 0.6095210 0.41884038  0.8870105
    ## 12         Denver , CO 0.2662216 0.09366149  0.7567031
    ## 13        Detroit , MI 0.6107936 0.47772119  0.7809342
    ## 14         Durham , NC 0.8979161 0.29634177  2.7206873
    ## 15     Fort Worth , TX 0.4708267 0.23532156  0.9420207
    ## 16         Fresno , CA 1.1967106 0.39113817  3.6614077
    ## 17        Houston , TX 0.7213297 0.55572903  0.9362775
    ## 18   Indianapolis , IN 0.9529549 0.68182153  1.3319073
    ## 19   Jacksonville , FL 0.7755568 0.56188327  1.0704863
    ## 20      Las Vegas , NV 0.8945580 0.62438928  1.2816267
    ## 21     Long Beach , CA 0.3971374 0.10531022  1.4976525
    ## 22    Los Angeles , CA 0.6547901 0.43835600  0.9780864
    ## 23     Louisville , KY 0.5081327 0.29964557  0.8616809
    ## 24        Memphis , TN 0.7509922 0.53576288  1.0526845
    ## 25          Miami , FL 0.4858555 0.25494504  0.9259075
    ## 26      Milwaukee , wI 0.7992289 0.53206084  1.2005523
    ## 27    Minneapolis , MN 1.1957101 0.47913258  2.9839810
    ## 28      Nashville , TN 0.9572701 0.60531972  1.5138546
    ## 29    New Orleans , LA 0.5759539 0.40190943  0.8253673
    ## 30       New York , NY 0.1922046 0.08831157  0.4183211
    ## 31        Oakland , CA 0.7101524 0.43900290  1.1487770
    ## 32  Oklahoma City , OK 1.1720440 0.71324541  1.9259670
    ## 33          Omaha , NE 0.6501622 0.30747614  1.3747764
    ## 34   Philadelphia , PA 0.5124027 0.38405944  0.6836351
    ## 35     Pittsburgh , PA 0.4498926 0.25418871  0.7962720
    ## 36       Richmond , VA 0.9819857 0.41211762  2.3398561
    ## 37     Sacramento , CA 0.3129805 0.11488649  0.8526399
    ## 38    San Antonio , TX 0.7708359 0.39815642  1.4923480
    ## 39 San Bernardino , CA 0.7423028 0.16107723  3.4208027
    ## 40      San Diego , CA 0.2369623 0.08885783  0.6319210
    ## 41  San Francisco , CA 0.6325958 0.30577673  1.3087244
    ## 42       Savannah , GA 1.0099168 0.36819643  2.7700754
    ## 43      St. Louis , MO 0.7143725 0.53071231  0.9615908
    ## 44       Stockton , CA 5.2404636 1.25480607 21.8858193
    ## 45          Tampa , FL 0.8245980 0.25507756  2.6657064
    ## 46          Tulsa , OK 0.9507531 0.54370182  1.6625499
    ## 47     Washington , DC 0.7497184 0.48954209  1.1481702

### plot of ORs and CIs for each city

``` r
p = ggplot(output, aes(x = reorder(city_state, or), y = or, fill = city_state)) +
  geom_bar(stat = "identity", color = "black", position = position_dodge()) +
  geom_errorbar(aes(ymin = ci_low, ymax = ci_high), width = .2, position = position_dodge(.9)) +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "city name", y = "estimated ORs", title = "ORs and CIs for each city")
print(p)
```

<img src="p8105_hw6_yz4433_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />

## Problem 3

### load and clean the data

``` r
bwdata =  read_csv(file = "./data/birthweight.csv", show_col_types = FALSE) %>% 
  janitor::clean_names() %>%
  mutate(babysex = factor(as.character(babysex)), frace = factor(as.character(frace)), malform = factor(as.character(malform)), mrace = factor(as.character(mrace)))
```

``` r
bwdata %>%
  summarise_all(
    ~ sum(is.na(.))
  )
```

    ## # A tibble: 1 × 20
    ##   babysex bhead blength   bwt delwt fincome frace gaweeks malform menarche
    ##     <int> <int>   <int> <int> <int>   <int> <int>   <int>   <int>    <int>
    ## 1       0     0       0     0     0       0     0       0       0        0
    ## # … with 10 more variables: mheight <int>, momage <int>, mrace <int>,
    ## #   parity <int>, pnumlbw <int>, pnumsga <int>, ppbmi <int>, ppwt <int>,
    ## #   smoken <int>, wtgain <int>
